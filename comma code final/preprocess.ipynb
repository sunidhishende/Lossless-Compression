{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dumping the filenames to `dataset.pkl` if it does not exist, else loading the entire dictionary from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from datasets import load_dataset, DatasetDict\n",
    "num_proc = 40\n",
    "ds_filename='dataset.pkl'\n",
    "splits = ['0', '1']\n",
    "if not os.path.exists(ds_filename):\n",
    "    ds = load_dataset('commaai/commavq', num_proc=num_proc, split=splits)\n",
    "    ds = DatasetDict(zip(splits, ds))\n",
    "    with open(ds_filename, 'wb') as f:\n",
    "        pickle.dump(ds, f)\n",
    "else:\n",
    "    with open(ds_filename, 'rb') as f:\n",
    "        ds =pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.save_to_disk('dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing to create dictionaries for each data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def create_nested_dict():\n",
    "    return {k: {i: [] for i in range(5000)} for k in range(128)}\n",
    "\n",
    "os.makedirs('commavq/dictionary_method_all_data/', exist_ok=True)\n",
    "file_template = 'commavq/dictionary_method_all_data/number_{}.pkl'\n",
    "\n",
    "# Initialize files (only need to do this once)\n",
    "for i in range(-1023, 1024):\n",
    "    temp_dict = create_nested_dict()\n",
    "    with open(file_template.format(i), 'wb') as f:\n",
    "        pickle.dump(temp_dict, f)\n",
    "\n",
    "# make a copy after this step runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing to store pixel data differences between adjacent frames instead of the frames itself (aiming that it compresses better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'commavq/data_0_to_5000_diff/'\n",
    "\n",
    "# Process data in larger chunks\n",
    "chunk_size = 25 # Adjust based on your available memory\n",
    "\n",
    "for split in ['0', '1']:\n",
    "    for chunk_start in range(0, 2500, chunk_size):\n",
    "        chunk_end = min(chunk_start + chunk_size, 2500)\n",
    "        chunk_dict = {i: create_nested_dict() for i in range(-1023, 1024)}\n",
    "        \n",
    "        for i in range(chunk_start, chunk_end):\n",
    "            print(f'i: {i}, split: {split}')\n",
    "            temp_arr = np.load(base_path + ds[split][i]['path'])\n",
    "            \n",
    "            for j in range(1200):\n",
    "                for k in range(128):\n",
    "                    row = k // 16\n",
    "                    col = k % 16\n",
    "                    number = temp_arr[j][row][col]\n",
    "                    if number != 0:\n",
    "                        chunk_dict[number][k][i].append(j)\n",
    "            \n",
    "            del temp_arr\n",
    "        \n",
    "        # Update files with chunk data\n",
    "        for number in range(-1023, 1024):\n",
    "            if any(chunk_dict[number][k][i] for k in range(128) for i in range(5000)):\n",
    "                with open(file_template.format(number), 'rb') as f:\n",
    "                    file_dict = pickle.load(f)\n",
    "                \n",
    "                for k in range(128):\n",
    "                    for i in range(5000):\n",
    "                        if chunk_dict[number][k][i]:\n",
    "                            file_dict[k][i].extend(chunk_dict[number][k][i])\n",
    "                \n",
    "                with open(file_template.format(number), 'wb') as f:\n",
    "                    pickle.dump(file_dict, f)\n",
    "        \n",
    "        del chunk_dict\n",
    "        gc.collect()\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "base_path = 'commavq/data_0_to_5000_diff/'\n",
    "\n",
    "counts = np.zeros(2047, dtype=np.int64)\n",
    "offset = 1023\n",
    "for split in ['0','1']:\n",
    "    for i in range(2500):\n",
    "        print(f'i: {i}, split: {split}')\n",
    "        temp_arr = np.load(base_path + ds[split][i]['path'])\n",
    "        counts += np.bincount(temp_arr.flatten() + offset, minlength=2047)\n",
    "        del temp_arr\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.linspace(-1023, 1023, 2047), counts)\n",
    "print(counts[0+offset])\n",
    "print(np.sum(counts)-counts[0+offset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import multiprocessing\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from pathlib import Path\n",
    "\n",
    "base_path = 'commavq/data_0_to_5000_diff/'\n",
    "ds_filename = 'dataset.pkl'\n",
    "splits = ['0', '1']\n",
    "num_proc = multiprocessing.cpu_count()\n",
    "\n",
    "if not os.path.exists(ds_filename):\n",
    "    ds = load_dataset('commaai/commavq', num_proc=num_proc, split=splits)\n",
    "    ds = DatasetDict(zip(splits, ds))\n",
    "    with open(ds_filename, 'wb') as f:\n",
    "        pickle.dump(ds, f)\n",
    "else:\n",
    "    with open(ds_filename, 'rb') as f:\n",
    "        ds =pickle.load(f)\n",
    "\n",
    "data = []\n",
    "for split in ds:\n",
    "    for min in ds[split]:\n",
    "        path = Path(base_path+ min['path'])\n",
    "        tokens=np.load(path)\n",
    "        data.append(tokens)\n",
    "\n",
    "data=np.array(data)\n",
    "data_shape = data.shape\n",
    "print(data_shape)\n",
    "\n",
    "output_dir='commavq/pixel_data_diff/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "data = data.reshape(5000, 1200, 128)\n",
    "# Iterate over each pixel position\n",
    "for pixel_idx in range(128):\n",
    "    print(\"pixel:\"+str(pixel_idx))\n",
    "    # Extract data for the current pixel position across all frames and minutes\n",
    "    pixel_data = data[:, :, pixel_idx].flatten()\n",
    "    file=f'pixel_{pixel_idx}.npy'\n",
    "    # Save the extracted data to a .npy file\n",
    "    np.save(output_dir+file, pixel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lzma\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "lzmaobj= lzma.LZMACompressor(preset=9)\n",
    "base_path='commavq/pixel_data_diff/'\n",
    "output_dir = Path('./temp/')\n",
    "os.makedirs(output_dir,exist_ok=True)\n",
    "\n",
    "for pixel_idx in tqdm(range(128)):\n",
    "    file=f'pixel_{pixel_idx}.npy'\n",
    "    tokens=np.load(base_path+file)\n",
    "    bytes=lzmaobj.compress(tokens.tobytes())\n",
    "    with open(output_dir/file,'wb') as f:\n",
    "        f.write(bytes)\n",
    "    print(\"pixel:\"+str(pixel_idx))\n",
    "\n",
    "shutil.make_archive('temp', 'zip', output_dir)\n",
    "final_zip= 'temp.zip'\n",
    "rate = (sum(ds.num_rows.values()) * 1200 * 128 * 10 / 8) / os.path.getsize(final_zip)\n",
    "print(f\"Compression rate: {rate:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'commavq/pixel_data_diff/'\n",
    "pixel_data = np.load(base_path+'pixel_0.npy')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "i = 1600\n",
    "plt.plot(pixel_data[i:i+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Path to the directory containing the .npy files\n",
    "data_dir = 'commavq/pixel_data_diff'\n",
    "\n",
    "# Initialize the zpaq archive\n",
    "archive_name = 'temp.zpaq'\n",
    "\n",
    "# Iterate over each .npy file in the directory\n",
    "for i in range(128):\n",
    "    file_name = f'pixel_{i}.npy'\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        # Create the command to add the file to the zpaq archive\n",
    "        command = f'zpaq add {archive_name} {file_path} -method 5'\n",
    "        \n",
    "        # Print the command (for debugging purposes)\n",
    "        print(f'Running command: {command}')\n",
    "        \n",
    "        # Run the command\n",
    "        result = subprocess.run(command, shell=True)\n",
    "        \n",
    "        # Check if the command was successful\n",
    "        if result.returncode != 0:\n",
    "            print(f'Error compressing {file_name}')\n",
    "    else:\n",
    "        print(f'{file_name} does not exist in the directory')\n",
    "\n",
    "print(\"Compression complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "base_path = 'commavq/data_0_to_5000/'\n",
    "temp_arr = np.load(base_path + ds['0'][0]['path'])\n",
    "\n",
    "np.set_printoptions(1100)\n",
    "np.set_printoptions(linewidth=200)\n",
    "\n",
    "i = 1\n",
    "print(temp_arr[i])\n",
    "print(temp_arr[i+1])\n",
    "print(temp_arr[i+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to the directory containing the files to compress\n",
    "base_path = 'commavq/data_0_to_5000/'\n",
    "\n",
    "# Initialize the zpaq archive\n",
    "archive_name = 'temp.zpaq'\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file_name in tqdm(os.listdir(base_path)):\n",
    "    file_path = os.path.join(base_path, file_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        # Create the command to add the file to the zpaq archive\n",
    "        command = f'zpaq add {archive_name} {file_path} -method 5'\n",
    "        \n",
    "        # Print the command (for debugging purposes)\n",
    "        # print(f'Running command: {command}')\n",
    "        \n",
    "        # Run the command\n",
    "        result = subprocess.run(command, shell=True)\n",
    "        \n",
    "        # Check if the command was successful\n",
    "        if result.returncode != 0:\n",
    "            print(f'Error compressing {file_name}')\n",
    "    else:\n",
    "        print(f'{file_name} is not a file')\n",
    "\n",
    "print(\"Compression complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_dir = 'commavq/pixel_data/'\n",
    "output_dir = 'commavq/pixel_data_6bits/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "template = 'pixel_{}.npy'\n",
    "\n",
    "def write_to_6bits(data):\n",
    "    temp = data[0]\n",
    "    new_data = []\n",
    "    count = 1\n",
    "    \n",
    "    for i in tqdm(range(1, len(data))):\n",
    "        if data[i] == temp and count <= 62:\n",
    "            count += 1\n",
    "        else:\n",
    "            encoded_value = (count << 10) + temp\n",
    "            new_data.append(encoded_value)\n",
    "            temp = data[i]\n",
    "            count = 1\n",
    "\n",
    "        \n",
    "    encoded_value = (count << 10) + temp\n",
    "    new_data.append(encoded_value)\n",
    "    \n",
    "    new_data = np.array(new_data, dtype=np.uint16)\n",
    "    return new_data\n",
    "\n",
    "for i in range(128):\n",
    "    file = template.format(i)\n",
    "    data = np.load(os.path.join(input_dir, file))\n",
    "    compressed_data = write_to_6bits(data)\n",
    "    np.save(os.path.join(output_dir, file), compressed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "number_of_files_in_one_merge = 128\n",
    "number_of_merges = 128//number_of_files_in_one_merge\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "\n",
    "data_dir = 'commavq/pixel_data/'\n",
    "output_dir = f'commavq/pixel_data_merged_{number_of_files_in_one_merge}/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize lists to hold the arrays for each merged file\n",
    "merged_data = []\n",
    "\n",
    "# Load and concatenate the arrays\n",
    "j = 0\n",
    "for i in range(number_of_merges):\n",
    "    merged_data = []\n",
    "    for k in range(number_of_files_in_one_merge):\n",
    "        file = f'pixel_{j}.npy'\n",
    "        data = np.load(os.path.join(data_dir, file))\n",
    "        merged_data.append(data)\n",
    "        j += 1\n",
    "    merged_data = np.concatenate(merged_data, axis=0)\n",
    "    output_file = f'pixel_merged_{i}.npy'\n",
    "    np.save(os.path.join(output_dir, output_file), merged_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to the directory containing the .npy files\n",
    "number_of_files_in_one_merge = 8\n",
    "number_of_merges = 128//number_of_files_in_one_merge\n",
    "data_dir = f'commavq/pixel_data_merged_{number_of_files_in_one_merge}/'\n",
    "\n",
    "# Initialize the zpaq archive\n",
    "archive_name = 'temp.zpaq'\n",
    "\n",
    "# Iterate over each .npy file in the directory\n",
    "for i in tqdm(range(number_of_merges)):\n",
    "    file_name = f'pixel_merged_{i}.npy'\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        # Create the command to add the file to the zpaq archive\n",
    "        command = f'zpaq add {archive_name} {file_path} -method x6.0w1c256ci1,1,1,1,1,1,2ac0,2,0,255i1c0,3,0,0,255i2c0,4,0,0,0,255i2mm20ts22t0'\n",
    "        \n",
    "        # Print the command (for debugging purposes)\n",
    "        print(f'Running command: {command}')\n",
    "            \n",
    "        # Run the command\n",
    "        result = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        \n",
    "        # Check if the command was successful\n",
    "        if result.returncode != 0:\n",
    "            print(f'Error compressing {file_name}')\n",
    "            print(result.stderr)\n",
    "            break\n",
    "    else:\n",
    "        print(f'{file_name} does not exist in the directory')\n",
    "\n",
    "print(\"Code complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lzma\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "with open('dataset.pkl', 'rb') as f:\n",
    "        ds =pickle.load(f)\n",
    "\n",
    "base_path='commavq/pixel_data_6bits/'\n",
    "output_dir = Path('./temp/')\n",
    "os.makedirs(output_dir,exist_ok=True)\n",
    "\n",
    "for pixel_idx in tqdm(range(128)):\n",
    "    lzmaobj=lzma.LZMACompressor(preset=9)\n",
    "    file=f'pixel_{pixel_idx}.npy'\n",
    "    tokens=np.load(base_path+file)\n",
    "    bytes=lzmaobj.compress(tokens.tobytes())\n",
    "    flush=lzmaobj.flush()\n",
    "    with open(output_dir/file,'wb') as f:\n",
    "        f.write(bytes)\n",
    "        f.write(flush)\n",
    "    print(\"pixel:\"+str(pixel_idx))\n",
    "\n",
    "shutil.make_archive('temp', 'zip', output_dir)\n",
    "final_zip= 'temp.zip'\n",
    "rate = (sum(ds.num_rows.values()) * 1200 * 128 * 10 / 8) / os.path.getsize(final_zip)\n",
    "print(f\"Compression rate:Â {rate:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pixel_data to pixel_bit_planes\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directory containing the pixel data files\n",
    "data_folder = 'commavq/pixel_data'\n",
    "\n",
    "# Create a directory to store the output\n",
    "output_folder = 'commavq/pixel_bit_planes_byte_arrays'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate over each pixel file\n",
    "for pixel_index in tqdm(range(128)):\n",
    "    pixel_file = os.path.join(data_folder, f'pixel_{pixel_index}.npy')\n",
    "    pixel_data = np.load(pixel_file)  # Load pixel data\n",
    "    \n",
    "    # Convert pixel data to boolean arrays for each bit\n",
    "    for bit_index in range(10):\n",
    "        # Extract the specific bit across all timesteps\n",
    "        bit_data = (pixel_data & (1 << bit_index)) > 0\n",
    "        # Save the boolean array to a file\n",
    "        output_file = os.path.join(output_folder, f'pixel_{pixel_index}_bit_{bit_index}.npy')\n",
    "        np.save(output_file, bit_data)\n",
    "\n",
    "print('Conversion complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_dir = 'commavq/pixel_data'\n",
    "output_dir = 'commavq/pixel_data_split'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i in tqdm(range(128)):\n",
    "    # Load the original file\n",
    "    file_path = os.path.join(input_dir, f'pixel_{i}.npy')\n",
    "    data = np.load(file_path)\n",
    "    \n",
    "    # Ensure data is of type int16\n",
    "    data = data.astype(np.int16)\n",
    "    \n",
    "    lower_bits = (data & 0x00FF).astype(np.int8)\n",
    "    upper_bits = ((data >> 8) & 0x00FF).astype(np.int8)\n",
    "    \n",
    "    # Save the lower and upper bits into separate files\n",
    "    lower_file_path = os.path.join(output_dir, f'pixel_{i}_lower.npy')\n",
    "    upper_file_path = os.path.join(output_dir, f'pixel_{i}_upper.npy')\n",
    "    \n",
    "    np.save(lower_file_path, lower_bits)\n",
    "    np.save(upper_file_path, upper_bits)\n",
    "\n",
    "print('All files processed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_dir = 'commavq/pixel_data_split'\n",
    "temp_arr = np.load(data_dir + '/pixel_0_lower.npy')\n",
    "i = 1000\n",
    "print(temp_arr[i:i+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to the directory containing the .npy files\n",
    "data_dir = 'commavq/pixel_data_split'\n",
    "\n",
    "# Initialize the zpaq archive\n",
    "archive_name = 'temp.zpaq'\n",
    "\n",
    "# Iterate over each .npy file in the directory\n",
    "for i in tqdm(range(128)):\n",
    "\n",
    "    lower_file_name = f'pixel_{i}_lower.npy'\n",
    "    upper_file_name = f'pixel_{i}_upper.npy'\n",
    "    lower_file_path = os.path.join(data_dir, lower_file_name)\n",
    "    upper_file_path = os.path.join(data_dir, upper_file_name)\n",
    "\n",
    "    if os.path.exists(lower_file_path) and os.path.exists(upper_file_path):\n",
    "        \n",
    "        command = f'zpaq add {archive_name} {lower_file_path} -method 5'\n",
    "        print(f'Running command: {command}')\n",
    "        result = subprocess.run(command, shell=True)\n",
    "        if result.returncode != 0:\n",
    "            print(f'Error compressing {lower_file_name}')\n",
    "\n",
    "        command = f'zpaq add {archive_name} {upper_file_path} -method 5'\n",
    "        print(f'Running command: {command}')\n",
    "        result = subprocess.run(command, shell=True)\n",
    "        if result.returncode != 0:\n",
    "            print(f'Error compressing {upper_file_name}')\n",
    "    else:\n",
    "        print(f'{lower_file_name} or {upper_file_name} does not exist in the directory')\n",
    "\n",
    "print(\"Compression complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the higher and lower splits\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "number_of_files_in_one_merge = 16\n",
    "number_of_merges = 128//number_of_files_in_one_merge\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "\n",
    "data_dir = 'commavq/pixel_data_split/'\n",
    "output_dir = f'commavq/pixel_data_split_merged_{number_of_files_in_one_merge}/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load and concatenate the arrays\n",
    "j = 0\n",
    "for i in range(number_of_merges):\n",
    "    merged_data_lower = []\n",
    "    merged_data_upper = []\n",
    "    for k in range(number_of_files_in_one_merge):\n",
    "        file_lower = f'pixel_{j}_lower.npy'\n",
    "        file_upper = f'pixel_{j}_upper.npy'\n",
    "        data_lower = np.load(os.path.join(data_dir, file_lower))\n",
    "        data_upper = np.load(os.path.join(data_dir, file_upper))\n",
    "        merged_data_lower.append(data_lower)\n",
    "        merged_data_upper.append(data_upper)\n",
    "        j += 1\n",
    "    merged_data_lower = np.concatenate(merged_data_lower, axis=0)\n",
    "    merged_data_upper = np.concatenate(merged_data_upper, axis=0)\n",
    "    output_file_lower = f'pixel_merged_{i}_lower.npy'\n",
    "    output_file_upper = f'pixel_merged_{i}_upper.npy'\n",
    "    np.save(os.path.join(output_dir, output_file_lower), merged_data_lower)\n",
    "    np.save(os.path.join(output_dir, output_file_upper), merged_data_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to the directory containing the .npy files\n",
    "number_of_files_in_one_merge = 16\n",
    "number_of_merges = 128//number_of_files_in_one_merge\n",
    "data_dir = f'commavq/pixel_data_split_merged_{number_of_files_in_one_merge}/'\n",
    "\n",
    "# Initialize the zpaq archive\n",
    "archive_name = 'temp.zpaq'\n",
    "\n",
    "# Iterate over each .npy file in the directory\n",
    "for i in tqdm(range(number_of_merges)):\n",
    "    file_name_lower = f'pixel_merged_{i}_lower.npy'\n",
    "    file_name_upper = f'pixel_merged_{i}_upper.npy'\n",
    "    file_path_lower = os.path.join(data_dir, file_name_lower)\n",
    "    file_path_upper = os.path.join(data_dir, file_name_upper)\n",
    "    if os.path.exists(file_path_lower) and os.path.exists(file_path_upper):\n",
    "        \n",
    "        command_lower = f'zpaq add {archive_name} {file_path_lower} -method 5'\n",
    "        print(f'Running command: {command_lower}')  \n",
    "        result = subprocess.run(command_lower, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        if result.returncode != 0:\n",
    "            print(f'Error compressing {file_name_lower}')\n",
    "            print(result.stderr)\n",
    "            break\n",
    "        \n",
    "        # command_upper = f'zpaq add {archive_name} {file_path_upper} -method 5'\n",
    "        # print(f'Running command: {command_upper}')\n",
    "        # result = subprocess.run(command_upper, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        # if result.returncode != 0:\n",
    "        #     print(f'Error compressing {file_name_upper}')\n",
    "        #     print(result.stderr)\n",
    "        #     break\n",
    "    else:\n",
    "        print(f'{file_name_lower} or {file_name_upper} does not exist in the directory')\n",
    "\n",
    "print(\"Code complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
